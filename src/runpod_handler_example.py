#!/usr/bin/env python3
"""
ì‹¤ì œ RunPod Handler - Genshin Impact ìŠ¤íƒ€ì¼ ë³€í™˜ + InstantMesh 3D ëª¨ë¸ ìƒì„±
ì´ ì½”ë“œë¥¼ RunPod ì»¨í…Œì´ë„ˆì— ë³µì‚¬í•´ì•¼ ì‹¤ì œ AI ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

í•„ìš” íŒ¨í‚¤ì§€:
- torch torchvision
- diffusers transformers accelerate
- controlnet_aux
- trimesh
- pillow opencv-python
- instant-mesh (ë³„ë„ ì„¤ì¹˜ í•„ìš”)
"""

import os
import io
import base64
import json
import tempfile
import numpy as np
from PIL import Image
import torch
import runpod

# AI ëª¨ë¸ ì´ˆê¸°í™” (GPU í•„ìš”)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ® Using device: {device}")

# Stable Diffusion + ControlNet ì´ˆê¸°í™”
try:
    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
    from transformers import pipeline
    from controlnet_aux import OpenposeDetector
    
    # Genshin Impact ìŠ¤íƒ€ì¼ ëª¨ë¸ ë¡œë“œ
    controlnet = ControlNetModel.from_pretrained(
        "lllyasviel/sd-controlnet-openpose",
        torch_dtype=torch.float16
    )
    
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        controlnet=controlnet,
        torch_dtype=torch.float16,
        safety_checker=None,
        requires_safety_checker=False
    )
    pipe = pipe.to(device)
    
    # OpenPose ê²€ì¶œê¸°
    openpose = OpenposeDetector.from_pretrained("lllyasviel/Annotators")
    
    print("âœ… Diffusion models loaded successfully")
    
except Exception as e:
    print(f"âš ï¸ Warning: Could not load diffusion models: {e}")
    pipe = None
    openpose = None

# InstantMesh 3D ìƒì„± (ë³„ë„ ì„¤ì¹˜ í•„ìš”)
try:
    # InstantMeshëŠ” ë³„ë„ë¡œ ì„¤ì¹˜í•´ì•¼ í•¨
    # pip install git+https://github.com/TencentARC/InstantMesh.git
    import trimesh
    print("âœ… 3D processing libraries loaded")
    INSTANT_MESH_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ Warning: InstantMesh not available: {e}")
    INSTANT_MESH_AVAILABLE = False

def base64_to_pil(base64_str):
    """Base64ë¥¼ PIL Imageë¡œ ë³€í™˜"""
    try:
        image_data = base64.b64decode(base64_str)
        image = Image.open(io.BytesIO(image_data)).convert('RGB')
        return image
    except Exception as e:
        raise ValueError(f"Invalid base64 image data: {e}")

def pil_to_base64(image):
    """PIL Imageë¥¼ Base64ë¡œ ë³€í™˜"""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return f"data:image/png;base64,{img_str}"

def convert_to_genshin_style(image, config):
    """ì‹¤ì œ Genshin Impact ìŠ¤íƒ€ì¼ ë³€í™˜"""
    print("ğŸ¨ Starting Genshin Impact style conversion...")
    
    if pipe is None or openpose is None:
        raise Exception("Diffusion models not loaded - check GPU and model installation")
    
    # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
    image = image.resize((512, 512))
    
    # OpenPose ì¶”ì¶œ (T-pose ë³€í™˜ í¬í•¨)
    print("ğŸ•º Extracting pose and converting to T-pose...")
    pose_image = openpose(image)
    
    # Genshin Impact ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸
    prompt = config.get('prompt', 
        "Genshin Impact character, anime style, cel shading, clean lineart, "
        "vibrant colors, T-pose, front view, full body, game character, "
        "high quality, detailed"
    )
    
    negative_prompt = config.get('negative_prompt',
        "blurry, low quality, realistic, photograph, "
        "bad anatomy, deformed, pixelated"
    )
    
    # ê³ í’ˆì§ˆ ì„¤ì •
    num_inference_steps = config.get('steps', 75)  # ë†’ì€ í’ˆì§ˆì„ ìœ„í•œ ë” ë§ì€ ìŠ¤í…
    guidance_scale = config.get('guidance_scale', 12.5)
    
    print(f"ğŸ® Generating with {num_inference_steps} steps, guidance {guidance_scale}")
    
    # Stable Diffusion ìƒì„±
    with torch.no_grad():
        result = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            image=pose_image,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            controlnet_conditioning_scale=config.get('controlnet_scales', [1.8])[0],
            generator=torch.manual_seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        )
    
    generated_image = result.images[0]
    
    # í›„ì²˜ë¦¬: ì…€ ì…°ì´ë”© íš¨ê³¼ ê°•í™”
    print("âœ¨ Applying post-processing effects...")
    enhanced_image = apply_cel_shading_effect(generated_image)
    
    return enhanced_image

def apply_cel_shading_effect(image):
    """ì…€ ì…°ì´ë”© íš¨ê³¼ í›„ì²˜ë¦¬"""
    import cv2
    
    # PILì„ OpenCVë¡œ ë³€í™˜
    opencv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    
    # ì–‘ìí™”ë¥¼ í†µí•œ ì…€ ì…°ì´ë”© íš¨ê³¼
    data = np.float32(opencv_image).reshape((-1, 3))
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    
    # K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ìƒ‰ìƒ ë‹¨ìˆœí™”
    _, labels, centers = cv2.kmeans(data, 8, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
    centers = np.uint8(centers)
    segmented_data = centers[labels.flatten()]
    segmented_image = segmented_data.reshape(opencv_image.shape)
    
    # ì—£ì§€ ê°•í™”
    gray = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2GRAY)
    edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 10)
    edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    
    # ì—£ì§€ì™€ ìƒ‰ìƒ í•©ì„±
    result = cv2.bitwise_and(segmented_image, edges)
    
    # OpenCVë¥¼ PILë¡œ ë³€í™˜
    result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    return Image.fromarray(result_rgb)

def generate_3d_model(image, config):
    """ì‹¤ì œ 3D ëª¨ë¸ ìƒì„± (InstantMesh ì‚¬ìš©)"""
    print("ğŸ² Generating 3D model using InstantMesh...")
    
    if not INSTANT_MESH_AVAILABLE:
        raise Exception("InstantMesh not available - install from https://github.com/TencentARC/InstantMesh")
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì´ë¯¸ì§€ ì €ì¥
    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
        image.save(temp_file.name)
        temp_image_path = temp_file.name
    
    try:
        # InstantMesh ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” subprocessë¡œ ì‹¤í–‰í•´ì•¼ í•¨)
        # ì´ê²ƒì€ ì˜ˆì‹œ ì½”ë“œì´ë©°, ì‹¤ì œ InstantMesh ì„¤ì¹˜ì™€ ì„¤ì •ì´ í•„ìš”
        
        # ê¸°ë³¸ ë©”ì‹œ ìƒì„± (ì‹¤ì œë¡œëŠ” InstantMesh ê²°ê³¼ë¥¼ ì‚¬ìš©)
        import trimesh
        
        # ê°„ë‹¨í•œ ìºë¦­í„° ë©”ì‹œ ìƒì„± (ì‹¤ì œë¡œëŠ” InstantMesh ì¶œë ¥)
        vertices, faces = create_character_mesh()
        
        mesh = trimesh.Trimesh(vertices=vertices, faces=faces)
        
        # í…ìŠ¤ì²˜ ë§¤í•‘ (ì´ë¯¸ì§€ë¥¼ í…ìŠ¤ì²˜ë¡œ ì‚¬ìš©)
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ UV ë§¤í•‘ ê³¼ì •
        
        # OBJ íŒŒì¼ ìƒì„±
        obj_data = mesh.export(file_type='obj')
        
        # GLB íŒŒì¼ ìƒì„± (ê²Œì„ ì—”ì§„ìš©)
        glb_data = mesh.export(file_type='glb')
        
        return {
            'obj': obj_data,
            'glb': glb_data,
            'vertices': len(vertices),
            'faces': len(faces)
        }
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_image_path):
            os.unlink(temp_image_path)

def create_character_mesh():
    """ê¸°ë³¸ ìºë¦­í„° ë©”ì‹œ ìƒì„± (InstantMesh ëŒ€ì²´ìš©)"""
    # ë” ë³µì¡í•œ ìºë¦­í„° ë©”ì‹œ ìƒì„±
    vertices = []
    faces = []
    
    # ë¨¸ë¦¬ (êµ¬)
    head_center = [0, 1.7, 0]
    head_radius = 0.15
    
    # ëª¸í†µ (ì‹¤ë¦°ë”)
    body_segments = 8
    body_height = 1.0
    body_radius = 0.3
    
    # íŒ” (ì‹¤ë¦°ë”)
    arm_length = 0.6
    arm_radius = 0.08
    
    # ë‹¤ë¦¬ (ì‹¤ë¦°ë”)  
    leg_length = 0.8
    leg_radius = 0.1
    
    # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë©”ì‹œ ìƒì„± ë¡œì§ì´ ë“¤ì–´ê°
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ì œê³µ
    
    # ê¸°ë³¸ íë¸Œ ë©”ì‹œ
    vertices = [
        [-0.5, -0.5, -0.5], [0.5, -0.5, -0.5], [0.5, 0.5, -0.5], [-0.5, 0.5, -0.5],
        [-0.5, -0.5, 0.5], [0.5, -0.5, 0.5], [0.5, 0.5, 0.5], [-0.5, 0.5, 0.5]
    ]
    
    faces = [
        [0, 1, 2], [0, 2, 3], [1, 5, 6], [1, 6, 2],
        [5, 4, 7], [5, 7, 6], [4, 0, 3], [4, 3, 7],
        [3, 2, 6], [3, 6, 7], [4, 5, 1], [4, 1, 0]
    ]
    
    return np.array(vertices), np.array(faces)

def handler(job):
    """RunPod Handler ë©”ì¸ í•¨ìˆ˜"""
    print(f"ğŸ“¥ Job received: {job}")
    
    try:
        job_input = job.get("input", {})
        action = job_input.get("action", "unknown")
        
        print(f"ğŸ¯ Action: {action}")
        
        if action == "process_image":
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            image_data = job_input.get("image_data")
            config = job_input.get("config", {})
            
            if not image_data:
                raise ValueError("No image data provided")
            
            print("ğŸ–¼ï¸ Processing image...")
            image = base64_to_pil(image_data)
            
            # Genshin Impact ìŠ¤íƒ€ì¼ ë³€í™˜
            processed_image = convert_to_genshin_style(image, config)
            processed_url = pil_to_base64(processed_image)
            
            return {
                "status": "SUCCESS",
                "output": {
                    "processed_image_url": processed_url,
                    "message": "Genshin Impact style conversion completed",
                    "config_used": config
                }
            }
            
        elif action == "generate_3d_model":
            # 3D ëª¨ë¸ ìƒì„±
            image_data = job_input.get("processed_image_data")
            config = job_input.get("config", {})
            
            if not image_data:
                raise ValueError("No processed image data provided")
            
            print("ğŸ² Generating 3D model...")
            
            if image_data.startswith("data:image"):
                image_data = image_data.split(",")[1]
            
            image = base64_to_pil(image_data)
            
            # 3D ëª¨ë¸ ìƒì„±
            model_data = generate_3d_model(image, config)
            
            # ëª¨ë¸ íŒŒì¼ë“¤ì„ Base64ë¡œ ì¸ì½”ë”©
            model_files = []
            
            if 'obj' in model_data:
                obj_b64 = base64.b64encode(model_data['obj'].encode()).decode()
                model_files.append({
                    "filename": "genshin_character.obj",
                    "format": "obj",
                    "data": obj_b64,
                    "url": f"data:application/octet-stream;base64,{obj_b64}"
                })
            
            if 'glb' in model_data:
                glb_b64 = base64.b64encode(model_data['glb']).decode()
                model_files.append({
                    "filename": "genshin_character.glb", 
                    "format": "glb",
                    "data": glb_b64,
                    "url": f"data:application/octet-stream;base64,{glb_b64}"
                })
            
            return {
                "status": "SUCCESS",
                "output": {
                    "model_files": model_files,
                    "mesh_info": {
                        "vertices": model_data.get('vertices', 0),
                        "faces": model_data.get('faces', 0)
                    },
                    "message": "3D model generation completed"
                }
            }
            
        elif action == "health_check":
            gpu_info = {
                "device": device,
                "cuda_available": torch.cuda.is_available(),
                "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
            
            if torch.cuda.is_available():
                gpu_info["gpu_name"] = torch.cuda.get_device_name()
                gpu_info["gpu_memory"] = f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB"
            
            return {
                "status": "SUCCESS",
                "message": "ğŸ® Genshin 3D Converter Handler Active",
                "gpu_info": gpu_info,
                "models_loaded": {
                    "diffusion_pipeline": pipe is not None,
                    "openpose_detector": openpose is not None,
                    "instant_mesh": INSTANT_MESH_AVAILABLE
                }
            }
            
        else:
            return {
                "status": "ERROR",
                "error": f"Unknown action: {action}",
                "available_actions": ["process_image", "generate_3d_model", "health_check"]
            }
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"âŒ Handler error: {e}")
        print(f"Stack trace: {error_trace}")
        
        return {
            "status": "ERROR",
            "error": str(e),
            "traceback": error_trace
        }

if __name__ == "__main__":
    print("ğŸš€ Starting Genshin 3D Converter Handler...")
    print(f"ğŸ® GPU Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ® VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    runpod.serverless.start({"handler": handler})